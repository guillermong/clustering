\chapter[Experimento]{Experimento}\label{ch:capitulo2}


\section{Diseño}\label{Diseño}

Las pruebas se realizaron comparando dos tipos de algoritmos de agrupamiento:

\begin{itemize}
  \item Algoritmo de agrupamiento propuesto
  \item Algoritmo de agrupamiento random
\end{itemize}

El algoritmo de agrupamiento random crea $n$ grupos y asigna de manera uniforme y distribuida aleatoriamente cada cadena de caracteres a un grupo, a diferencia del algoritmo propuesto que luego de obtener las muestras es determinista. También  mantiene el balance de espacio en memoria en cada Grupo. 

Con ambos algoritmos se pretende demostrar que realizando los agrupamientos de manera inteligente se pueden obtener mejores resultados en términos de compresión, que agrupándolos aleatoriamente y mantener cierto balance en cada agrupación.

En el algoritmo de agrupamiento propuesto existen distintas variables que pueden determinar un buen agrupamiento de la colección, estas son:


\begin{itemize}
  \item Cantidad de Grupos: es difícil determinar la cantidad exacta de grupos que se necesitan para lograr la mejor compresión. En este caso el número representa la cantidad de máquinas disponibles.
  \item Tamaño de la muestra: si la muestra es muy pequeña, es probable que no represente todos los tipos de grupos que se encuentra en la colección. En caso contrario, si la muestra es muy grande el tiempo de ejecución crece.
  \item Medida de distancia: la función implementada para medir la distancia permite cambiar el método de comprimir, en este caso se utiliza ZIP. También la librería de ZIP utilizada permite determinar el nivel de compresión  de una cadena de caracteres, cambiando los valores en la medida de distancia, entre mayor sea el nivel de la compresión se obtiene una mejor calidad de la distancia pero más lento poder calcularla.
  

\end{itemize}

Las pruebas se realizaron en una colección versión en español de  Wikipedia, las muestras se tomaron de manera uniformemente al azar, en UTF-8. Para cada documento seleccionado se obtienen todas sus versiones.  Esto fue hecho usando la libreria go-wikiparse y limpiado con Tika para obtener sólo el texto de los artículos. Se concatenan todos los documentos en uno sólo y se divide por bloques. 

\begin{table}[H]
\begin{center}
\resizebox{15cm}{!} {

\begin{tabular}{|p{3cm}|p{3cm}||p{3cm}||p{3cm}||p{3cm}|}

\hline
Colecciones & Tamaño Total (GiB)  & Entropia(bits)  & Nº Documentos  & Tamaño documento (MiB) \\
\hline
Wiki-ES  & 16.384  & 5.0831497879 & 16384 & 1 \\
\hline
\end{tabular}
}
\end{center}
\caption{Colecciones.}

\label{Colecciones}

\end{table}	


\section{Resultados}\label{Resultados}

\subsection{Resultados iniciales}\label{result iniciales}

En las pruebas se tomó una muestra de 30 documentos y en esta primera etapa las pruebas se ejecutaron solamente una vez en cada caso, a excepción del algoritmo de agrupamiento random que se ejecutaron diez veces. La muestra se obtuvo aleatoriamente de la colección de documentos, manteniéndose estas para todos los casos. Como la muestra es insignificante, en comparación al tamaño de la colección, es muy probable que ningún documentos pertenezca a uno de la misma versión. Este problema origina que la mayoría de los documentos pertenezca a un solo grupo y el resto solamente es representado por un documento, porque si un grupo crece es muy probable que al intentar unir dos grupos, el grupo más grande se una a otro grupo. También cabe mencionar que la elección de la cantidad de agrupaciones es arbitraria, pero la cantidad de agrupaciones es una variable importante al momento de obtener buenos resultados en las agrupaciones. En este caso las pruebas se realizaron con un número fijo de agrupaciones para observar el comportamiento de otras variables que afectan a las agrupaciones.

En la tabla \ref{Resultado algoritmo de agrupamiento aleatorio con 10 Grupos.}  muestra los resultados  de cada método con una cantidad de 10 agrupaciones. El \textit{Método 1}  utiliza el algoritmo de agrupación aleatoria, que en cada grupo comprimido y no comprimido se mantiene una carga de almacenamiento balanceada que es uno de los objetivos deseados en la memoria. En los métodos siguientes se utiliza el algoritmo de agrupación propuesto pero modificando algunas variables para observar su comportamiento.

Para el caso del \textit{Método 2}  se observa una mejora de la compresión equivalente al  45\% del tamaño total del resultado en el algoritmo de agrupamiento aleatorio, aquí la muestra es de 30 documentos. En términos de balance en la carga de almacenamiento que se representa en el $Error$ de la tabla~\ref{distribucion}, este método es ineficiente, ya que la mayor parte de la carga se concentra solamente en un grupo. Esto se debe a que en el momento de crear los grupos con las muestras, la mayor parte de las muestra quedan solamente en un grupo, dejando a las demás con pocas muestras de representación.

En el \textit{Método 3} se observa que existe un balance en la cantidad de muestras en cada agrupación. Para esto, cada grupo no tendrá una muestra superior a tres en un universo de 30 documentos. Con esto se busca balancear la cantidad de documentos en cada agrupación.
El resultado de la compresión, utilizando el \textit{Método 3}, es equivalente al 55\% del tamaño total del resultado con el método del algoritmo de agrupamiento aleatorio, que sigue siendo una mejor alternativa, pero comparando con los resultado del \textit{Método 2} se paga un costo al balancear las muestras en los grupos, de un 24\% más del tamaño total del resultado en el \textit{Método 2}.

En el \textit{Método 4}, se hace la misma prueba que en el método anterior,  pero se agrega la condición de que el tamaño de los grupos no supere determinado límite, el que en este caso es el tamaño de la colección de datos divido por la cantidad de agrupaciones. Con esta medida se asegura que en todos los grupos tengan, aproximadamente, la misma cantidad de cadenas de caracteres.  El resultado del \textit{Método 4} es el equivalente al 70\% del tamaño total del resultado en el algoritmo de agrupamiento aleatorio, que sigue siendo una mejora, pero nuevamente pagando un costo, con respecto al \textit{Método 2} aumenta 55\% más de tamaño, incluso mayor que en el \textit{Método 3}, pero con mejores resultados en el balance de la carga de almacenamiento.

%------------------------------------------------------%


\begin{table}[H]
\begin{center}
\resizebox{15cm}{!} {

\begin{tabular}{|p{3cm}|p{3cm}||p{3cm}||p{3cm}||p{3cm}|}

\hline
Grupos & Método 1(KiB)  & Método 2(KiB)  & Método 3(KiB)  & Método 4(KiB) \\
\hline
Total  & 65.560 & 29.371 (45\%) & 36.036 (55\%) & 45.639 (70\%) \\
\hline
\end{tabular}
}
\end{center}
\caption{Resultado algoritmo de agrupamiento aleatorio con 10 Grupos.}

\label{Resultado algoritmo de agrupamiento aleatorio con 10 Grupos.}

\end{table}	

En la tabla ~\ref{distribucion} se observa el resultado de la distribución de los datos de cada grupo utilizando los métodos mencionados.

Para determinar si la carga de almacenamiento en todos los grupos se encuentra balanceada en términos del número de documentos, se calcula el promedio del error absoluto $\bar{E_{a}}$, que se define como~\ref{errorabs}:

  \begin{equation}
  \begin{aligned}
		\bar{E_{a}} &=\frac{1}{k}\sum_{i=1}^{k} \mid V_{verdadero} - V_{i}\mid
  \end{aligned}\label{errorabs}
	\end{equation}
	
donde $V_{verdadero}=\frac{\varrho}{k}$ con $\varrho$ el número de documentos en la Colección, $k$ es el número de grupos formados por el algoritmo de agrupamiento y $V_{i}$ la cantidad de documentos en un grupo. $\bar{T}$ representa el promedio de la carga de almacenamiento que ocupan los grupos. Por último, $\bar{S}$ es el promedio de la distancia entre los documentos de las muestras de un mismo grupo , y se define como~\ref{simi}: 

\begin{equation}
  \begin{aligned}
		\bar{S} &=\frac{1}{k}\sum_{i=1}^{k} Distancia(G_{i})\\
  \end{aligned}\label{simi}
\end{equation}

donde  $Distancia(G_{i})$ es el promedio de la distancia entre los documentos del mismo grupos.

\begin{table}[H]
\begin{center}
\resizebox{15cm}{!} {

\begin{tabular}{|p{3cm}|p{3cm}||p{3cm}||p{3cm}|}

\hline
Grupos & $\bar{E_{a}}$  & $\bar{T} (KiB)$  & $\bar{S}$ \\
\hline
Método 1 &  0.2  & 6553.6 &  - \\
\hline
Método 2 & 1910.4  & 2937.2 & 0.196718 \\
\hline
Método 3 & 509 & 3603.7 & 0.994174 \\
\hline
Método 4 & 1.4 & 4563.9 & 0.994174  \\
\hline
\end{tabular}
}
\end{center}
\caption{Distribución grupos.}

\label{distribucion}

\end{table}	



\subsection{Resultados Finales}\label{result finales}

En esta segunda etapa de la experimentación se ejecutó el algoritmo de agrupamiento propuesto, con el objetivo de observar qué tan eficiente es el algoritmo con las mejoras implementadas, ver \ref{mejoras123}, comparándolos con el algoritmo propuesto inicial. Además de ver qué tan determinante es la elección de las muestras de manera aleatoria en el resultado de la compresión y, determinar qué ocurre cuando se aumentan las muestras.

Las muestras que se utilizaron fueron de 10, 20, 30, 40, 50. Para los casos del algoritmo con las mejoras implementadas se ejecutaron pruebas con muestras de 10, 30, 50 y 100. En cada caso se ejecutaron 10 veces, a excepción del caso con 100 muestras el que se ejecutaron cinco veces.

En el gráfico \ref{fig:grafico1} muestra el total del tamaño en cada prueba realizada, en él se puede observa que aumentando las muestras mejoran los resultados de la compresión. Además, con las mejoras implementadas se puede observar que se tienen mejores resultados en la compresión que en los resultados sin las mejoras implementadas, por ejemplo, si se compara con las pruebas con 30 muestras se observa que el resultado con la mejora implementada representa un 86\% del resultado del algoritmo sin la mejora implementada. Otro punto que se puede observar del gráfico es cómo varían los resultados con el cambio de muestras, con esto se puede deducir que las muestras pueden influir en algún grado en el resultado de la compresión.


\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{./Figuras/grafico1}
\caption{Legos.} \label{fig:grafico1}
\end{figure}


En el gráfico \ref{fig:grafico2:a} muestra el promedio, el mínimo  y el máximo tamaño entre las 10 pruebas para cada caso realizado con el algoritmo sin las mejoras implementadas. Se observa que aumentando las muestras, hasta llegar a las 30, la compresión de los datos baja y luego se mantienen estable. En cambio en el gráfico \ref{fig:grafico2:b} la compresión sigue mejorando luego de las 30 muestras y el rango entre el máximo y el mínimo cada vez es menor, lo que significa que las muestras tomadas aleatoriamente influyen menos en la compresión.

Comparando con los resultados iniciales \ref{result iniciales}, el algoritmo con las mejoras implementadas, si se ejecuta con 150 muestras  se puede conseguir casi el mismo resultado que se obtuvo del \textit{Método 2}(versión del algoritmo sin ninguna limitación y mejora) con tan solo 30 muestras.



\begin{table}[H]
\begin{center}
\resizebox{15cm}{!} {

\begin{tabular}{|p{2cm}|p{2cm}||p{2cm}||p{2cm}||p{2cm}||p{2cm}||p{2cm}||p{2cm}||p{2cm}||p{2cm}|}

\hline
Pruebas & $S_{10}$  & $S_{20}$  & $S_{30}$ & $S_{40}$ & $S_{50}$ & $S_{10cola}$ & $S_{30_cola}$ & $S_{50cola}$ & $\bar{S_{100cola}}$\\
\hline
Promedio & 50506,9	& 49171,6 &	46216,5 & 45879,7 & 45343,1 & 43800,7 & 39956,4 & 38220,2 & 34820,6  \\
\hline
Mínimo & 47852 & 46626 & 43017 & 42713 & 42783 & 41112 & 38003 & 37165 & 33943  \\
\hline
Máximo & 53468 & 50990 & 48537 & 48950 & 48274 & 46449 & 42242 & 39861 & 36237
 \\
\hline
Random & 77,04\% & 75,00\% & 70,49\% & 69,98\% & 69,16\% & 66,81\% & 60,95\% & 58,30\% & 53,11\%
  \\
\hline
\end{tabular}
}
\end{center}
\caption{Distribución grupos.}

\label{distribucion}

\end{table}	



\begin{figure}[htbp]
\centering
\subfigure[Sin Cola \label{fig:grafico2:a}]{\includegraphics[width=70mm]{./Figuras/grafico2}}\hspace{10mm}
\subfigure[Con cola \label{fig:grafico2:b}]{\includegraphics[width=70mm]{./Figuras/grafico3}}\vspace{10mm}
\caption{Legos.} \label{fig:grafico2}
\end{figure}
